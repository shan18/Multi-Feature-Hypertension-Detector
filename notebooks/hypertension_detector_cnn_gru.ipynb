{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hypertension_detector_cnn_gru.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43IeGp8O1z2J"
      },
      "source": [
        "### Get Dataset from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jlPlHPo1Mqu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLpv1CWQ1g5b"
      },
      "source": [
        "!cp '/content/drive/My Drive/physiobank_dataset.json' ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ztbi1sj13F8"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DylbGm141xO6"
      },
      "source": [
        "import json\n",
        "import random\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from utils import ProgressBar\n",
        "from dataset import PhysioBank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD3eNhi818_X"
      },
      "source": [
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdCzfM9V2-AS"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTPIqMeG6H-a"
      },
      "source": [
        "class PhysioBank:\n",
        "    def __init__(\n",
        "        self, path, train_batch_size=1, val_batch_size=1, test_batch_size=1,\n",
        "        cuda=False, num_workers=1, train_split=0.7, val_split=0.15, mean=78.78, std=28.35\n",
        "    ):\n",
        "        \"\"\"Initializes the dataset for loading.\"\"\"\n",
        "\n",
        "        self.path = path\n",
        "        self.cuda = cuda\n",
        "        self.num_workers = num_workers\n",
        "        self.train_split = train_split\n",
        "        self.val_split = val_split\n",
        "        self.train_batch_size = train_batch_size\n",
        "        self.val_batch_size = val_batch_size\n",
        "        self.test_batch_size = test_batch_size\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "        # Get data\n",
        "        self._create_data(self._read_data())\n",
        "    \n",
        "    def _read_data(self):\n",
        "        with open(self.path) as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "    \n",
        "    def _get_normalization(self, samples):\n",
        "        self.transition, self.scale = {}, {}\n",
        "\n",
        "        # IHR\n",
        "        samples_ihr = [y for x in samples for y in x['ihr']]\n",
        "        self.transition['ihr'] = min(samples_ihr)\n",
        "        self.scale['ihr'] = max(samples_ihr) - self.transition['ihr']\n",
        "\n",
        "        # Age\n",
        "        samples_age = [x['age'] for x in samples]\n",
        "        self.transition['age'] = min(samples_age)\n",
        "        self.scale['age'] = max(samples_age) - self.transition['age']\n",
        "\n",
        "    def _create_data(self, samples):\n",
        "        random.shuffle(samples)\n",
        "\n",
        "        # Calculate number of samples in each set\n",
        "        train_limit = int(len(samples) * self.train_split)\n",
        "        val_limit = int(len(samples) * self.val_split)\n",
        "\n",
        "        # Distribute data\n",
        "        self._get_normalization(samples[:train_limit])\n",
        "        self.train_data = PhysioBankDataset(samples[:train_limit], self.transition, self.scale)\n",
        "        self.val_data = PhysioBankDataset(samples[train_limit:train_limit + val_limit], self.transition, self.scale)\n",
        "        self.test_data = PhysioBankDataset(samples[train_limit + val_limit:], self.transition, self.scale)\n",
        "\n",
        "    def loader(self, type='train', shuffle=True):\n",
        "        loader_args = { 'shuffle': shuffle }\n",
        "\n",
        "        # If GPU exists\n",
        "        if self.cuda:\n",
        "            loader_args['num_workers'] = self.num_workers\n",
        "            loader_args['pin_memory'] = True\n",
        "\n",
        "        if type == 'train':\n",
        "            loader_args['batch_size'] = self.train_batch_size\n",
        "            return DataLoader(self.train_data, **loader_args)\n",
        "        elif type == 'val':\n",
        "            loader_args['batch_size'] = self.val_batch_size\n",
        "            return DataLoader(self.val_data, **loader_args)\n",
        "        else:\n",
        "            loader_args['batch_size'] = self.test_batch_size\n",
        "            return DataLoader(self.test_data, **loader_args)\n",
        "\n",
        "\n",
        "class PhysioBankDataset(Dataset):\n",
        "    def __init__(self, samples, transition, scale):\n",
        "        \"\"\"Initializes the dataset for loading.\"\"\"\n",
        "        super(PhysioBankDataset, self).__init__()\n",
        "        self.samples = samples\n",
        "        self.transition = transition\n",
        "        self.scale = scale\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns length of the dataset.\"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.samples[index]\n",
        "\n",
        "        return (\n",
        "            (\n",
        "                (torch.FloatTensor(sample['ihr']) - self.transition['ihr']) / self.scale['ihr'],\n",
        "                torch.FloatTensor([\n",
        "                    sample['gender'],\n",
        "                    (sample['age'] - self.transition['age']) / self.scale['age']\n",
        "                ])\n",
        "            ),\n",
        "            torch.FloatTensor([sample['hypertensive']])\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vHG8YkY3A1B"
      },
      "source": [
        "dataset = PhysioBank(\n",
        "    'physiobank_dataset.json',\n",
        "    train_batch_size=128,\n",
        "    val_batch_size=128,\n",
        "    test_batch_size=128,\n",
        "    cuda=torch.cuda.is_available()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "illBL85z3TYj"
      },
      "source": [
        "train_loader = dataset.loader(type='train')\n",
        "val_loader = dataset.loader(type='val')\n",
        "test_loader = dataset.loader(type='test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H64c1t4d3bBs"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKeaFE1RhoCi"
      },
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = self._create_conv_sequence(input_dim, output_dim, 128, dropout)\n",
        "        self.conv2 = self._create_conv_sequence(input_dim, output_dim, 256, dropout)\n",
        "        self.conv3 = self._create_conv_sequence(input_dim, output_dim, 512, dropout)\n",
        "\n",
        "        self.pointwise = nn.Conv1d(output_dim * 3, output_dim, 1)\n",
        "    \n",
        "    def _create_conv_sequence(self, input_dim, output_dim, kernel_size, dropout):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(input_dim, output_dim, kernel_size, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    \n",
        "    def forward(self, seq):\n",
        "        features1 = self.conv1(seq)\n",
        "        features2 = self.conv2(seq)\n",
        "        features3 = self.conv3(seq)\n",
        "\n",
        "        features = torch.cat((features1, features2, features3), dim=1)\n",
        "\n",
        "        features = self.pointwise(features)\n",
        "\n",
        "        return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozMuT4rBBhlv"
      },
      "source": [
        "class HypertensionDetectorConvGRU(nn.Module):\n",
        "    def __init__(self, feature_dim, hidden_dim, seq_meta_len, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.seq_meta_fc = nn.Linear(seq_meta_len, hidden_dim)\n",
        "\n",
        "        self.conv1 = ConvBlock(1, feature_dim, dropout=dropout)\n",
        "        self.conv2 = ConvBlock(feature_dim, feature_dim, dropout=dropout)\n",
        "        self.conv3 = ConvBlock(feature_dim, feature_dim, dropout=dropout)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "\n",
        "        self.rnn = nn.GRU(feature_dim, hidden_dim, num_layers=n_layers, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(2 * n_layers * hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    def forward(self, seq, seq_meta):\n",
        "        \"\"\"Input shapes\n",
        "\n",
        "        seq: [batch_size, seq_len]\n",
        "        seq_meta: [batch_size, seq_meta_len]\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = seq.shape\n",
        "\n",
        "        seq = seq.unsqueeze(1)  # [batch_size, 1, seq_len]\n",
        "\n",
        "        features = self.conv1(seq)  # [batch_size, feature_dim, seq_len]\n",
        "        features = self.pool(features)\n",
        "        features = self.conv2(features)  # [batch_size, feature_dim, seq_len / 2]\n",
        "        features = self.pool(features)\n",
        "        features = self.conv3(features)  # [batch_size, feature_dim, seq_len / 4]\n",
        "\n",
        "        features = features.permute(2, 0, 1)  # [seq_len / 4, batch_size, feature_dim]\n",
        "\n",
        "        seq_meta = self.seq_meta_fc(seq_meta)  # [batch_size, hidden_dim]\n",
        "        seq_meta = seq_meta.unsqueeze(0).repeat(self.n_layers * 2, 1, 1)  # [n_layers * 2, batch_size, hidden_dim]\n",
        "\n",
        "        _, hidden = self.rnn(\n",
        "            features, seq_meta\n",
        "        )  # [2 * num_layers, batch_size, hidden_dim]\n",
        "\n",
        "        hidden = hidden.permute(1, 0, 2).reshape(batch_size, -1)  # [batch_size, 2 * num_layers * hidden_dim]\n",
        "\n",
        "        output = self.fc1(hidden)  # [batch_size, hidden_dim]\n",
        "        output = self.fc2(output)  # [batch_size, 1]\n",
        "    \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflaNuR60F45"
      },
      "source": [
        "model = HypertensionDetectorConvGRU(\n",
        "    64, 128, dataset.train_data[0][0][1].shape[0], 2, 0.1\n",
        ").to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xygqAgfyEzjc",
        "outputId": "68ef245e-9120-4e8a-aeaf-b7416c9c1198"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 7,946,625 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxggEAds3iLJ"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnVgy4bCzIG5"
      },
      "source": [
        "Create optimizer and criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ghJ1V9RZ3vp"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkGmx9HjzsWv"
      },
      "source": [
        "Define training and validation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIMCH8GB3jy0"
      },
      "source": [
        "def train(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    pbar = ProgressBar(target=len(loader), width=8)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "\n",
        "    for batch_idx, data in enumerate(loader, 0):\n",
        "        (source, source_meta), target  = data\n",
        "        source = source.to(device)\n",
        "        source_meta = source_meta.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(source, source_meta)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        pred = (output > 0.5).float()\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(target)\n",
        "        accuracy = 100 * correct / processed\n",
        "\n",
        "        pbar.update(batch_idx, values=[\n",
        "            ('Loss', round(loss.item(), 2)), ('Accuracy', round(accuracy, 2))\n",
        "        ])\n",
        "    \n",
        "    pbar.add(1, values=[\n",
        "        ('Loss', round(loss.item(), 2)), ('Accuracy', round(accuracy, 2))\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JInmMo_8JyO"
      },
      "source": [
        "def val(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (source, source_meta), target in loader:\n",
        "            source = source.to(device)\n",
        "            source_meta = source_meta.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(source, source_meta)\n",
        "\n",
        "            cost = criterion(output, target)\n",
        "            loss += cost.item()\n",
        "\n",
        "            pred = (output > 0.5).float()\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    \n",
        "    loss /= len(loader)\n",
        "    accuracy = correct / len(loader)\n",
        "    print(\n",
        "        f'Validation set: Average loss: {loss:.4f}, Accuracy: {accuracy:.2f}%\\n'\n",
        "    )\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuVRew7mwEl_",
        "outputId": "8055f6a6-1cab-4956-e0d4-be10cfed628c"
      },
      "source": [
        "epochs = 20\n",
        "best_val_accuracy = 0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f'Epoch {epoch}:')\n",
        "    train(model, train_loader, optimizer, criterion, device)\n",
        "    accuracy = val(model, val_loader, criterion, device)\n",
        "\n",
        "    if accuracy > best_val_accuracy:\n",
        "        print(f'Validation accuracy improved from {best_val_accuracy:.2f}% to {accuracy:.2f}%\\n')\n",
        "        best_val_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), 'hypertension_detector.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.6800 - Accuracy: 45.6712\n",
            "Validation set: Average loss: 0.6775, Accuracy: 52.00%\n",
            "\n",
            "Validation accuracy improved from 0.00% to 52.00%\n",
            "\n",
            "Epoch 2:\n",
            "16/16 [========] - 172s 11s/step - Loss: 0.6500 - Accuracy: 47.9050\n",
            "Validation set: Average loss: 0.6306, Accuracy: 64.50%\n",
            "\n",
            "Validation accuracy improved from 52.00% to 64.50%\n",
            "\n",
            "Epoch 3:\n",
            "16/16 [========] - 172s 11s/step - Loss: 0.5850 - Accuracy: 63.5138\n",
            "Validation set: Average loss: 0.6231, Accuracy: 73.75%\n",
            "\n",
            "Validation accuracy improved from 64.50% to 73.75%\n",
            "\n",
            "Epoch 4:\n",
            "16/16 [========] - 172s 11s/step - Loss: 0.5400 - Accuracy: 69.0825\n",
            "Validation set: Average loss: 0.5208, Accuracy: 80.50%\n",
            "\n",
            "Validation accuracy improved from 73.75% to 80.50%\n",
            "\n",
            "Epoch 5:\n",
            "16/16 [========] - 172s 11s/step - Loss: 0.5013 - Accuracy: 72.5419\n",
            "Validation set: Average loss: 0.5501, Accuracy: 82.50%\n",
            "\n",
            "Validation accuracy improved from 80.50% to 82.50%\n",
            "\n",
            "Epoch 6:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4812 - Accuracy: 76.8137\n",
            "Validation set: Average loss: 0.4669, Accuracy: 84.25%\n",
            "\n",
            "Validation accuracy improved from 82.50% to 84.25%\n",
            "\n",
            "Epoch 7:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4381 - Accuracy: 77.7438\n",
            "Validation set: Average loss: 0.4554, Accuracy: 83.50%\n",
            "\n",
            "Epoch 8:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4194 - Accuracy: 78.4681\n",
            "Validation set: Average loss: 0.4432, Accuracy: 83.50%\n",
            "\n",
            "Epoch 9:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4200 - Accuracy: 77.1163\n",
            "Validation set: Average loss: 0.4372, Accuracy: 84.00%\n",
            "\n",
            "Epoch 10:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4281 - Accuracy: 78.7381\n",
            "Validation set: Average loss: 0.4364, Accuracy: 84.75%\n",
            "\n",
            "Validation accuracy improved from 84.25% to 84.75%\n",
            "\n",
            "Epoch 11:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4188 - Accuracy: 78.0163\n",
            "Validation set: Average loss: 0.4567, Accuracy: 84.25%\n",
            "\n",
            "Epoch 12:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4181 - Accuracy: 78.1412\n",
            "Validation set: Average loss: 0.4860, Accuracy: 79.75%\n",
            "\n",
            "Epoch 13:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4219 - Accuracy: 78.3219\n",
            "Validation set: Average loss: 0.4351, Accuracy: 83.75%\n",
            "\n",
            "Epoch 14:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4244 - Accuracy: 76.4775\n",
            "Validation set: Average loss: 0.4625, Accuracy: 85.75%\n",
            "\n",
            "Validation accuracy improved from 84.75% to 85.75%\n",
            "\n",
            "Epoch 15:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4213 - Accuracy: 78.0681\n",
            "Validation set: Average loss: 0.4265, Accuracy: 83.75%\n",
            "\n",
            "Epoch 16:\n",
            "16/16 [========] - 173s 11s/step - Loss: 0.4244 - Accuracy: 78.1425\n",
            "Validation set: Average loss: 0.4298, Accuracy: 85.50%\n",
            "\n",
            "Epoch 17:\n",
            "16/16 [========] - 174s 11s/step - Loss: 0.4100 - Accuracy: 76.1506\n",
            "Validation set: Average loss: 0.4449, Accuracy: 85.25%\n",
            "\n",
            "Epoch 18:\n",
            "16/16 [========] - 174s 11s/step - Loss: 0.4088 - Accuracy: 78.6756\n",
            "Validation set: Average loss: 0.4351, Accuracy: 83.75%\n",
            "\n",
            "Epoch 19:\n",
            "16/16 [========] - 174s 11s/step - Loss: 0.4169 - Accuracy: 77.7662\n",
            "Validation set: Average loss: 0.4789, Accuracy: 83.00%\n",
            "\n",
            "Epoch 20:\n",
            "16/16 [========] - 174s 11s/step - Loss: 0.4138 - Accuracy: 78.0563\n",
            "Validation set: Average loss: 0.4344, Accuracy: 83.75%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDKGWj_UO7i7"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S78iMRrHPShv",
        "outputId": "abcd79fb-1dc9-4b57-b501-563a84ecb0a0"
      },
      "source": [
        "model.load_state_dict(torch.load('hypertension_detector.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzRIOlAVMS6n",
        "outputId": "b4eeef8b-1635-4eff-86ad-e200092b6732"
      },
      "source": [
        "_ = val(model, test_loader, criterion, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: Average loss: 0.4323, Accuracy: 85.25%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otEha0_8P-5h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}